---
layout: dm_csml_seminar_event
posted_date: 2020-03-01
event_start_time: 2021-01-29 14:00
event_end_time: 2021-01-29 15:00
img: ellis-logo.png
alt: image-alt
join_link: Zoom
location: Zoom
speaker: Guido Montufar
affiliation: Max Planck Institute for Mathematics in the Sciences
title: Implicit bias of gradient descent for mean squared error regression with wide neural networks
summary: <p>We investigate gradient descent training of wide neural networks and the corresponding implicit bias in function space. For 1D regression, we show that the solution of training a width-$n$ shallow ReLU network is within $n^{- 1/2}$ of the function which fits the training data and whose difference from initialization has smallest 2-norm of the second derivative weighted by $1/\zeta$. The curvature penalty function $1/\zeta$ is expressed in terms of the probability distribution that is utilized to initialize the network parameters, and we compute it explicitly for various common initialization procedures. For instance, asymmetric initialization with a uniform distribution yields a constant curvature penalty, and hence the solution function is the natural cubic spline interpolation of the training data. While similar results have been obtained in previous works, our analysis clarifies important details and allows us to obtain significant generalizations. In particular, the result generalizes to multivariate regression and different activation functions. Moreover, we show that the training trajectories are captured by trajectories of spatially adaptive smoothing splines with decreasing regularization strength. This is joint  work with Hui Jin (UCLA).</p>
icalendar: /ics/id/420
calendar_name: csml_id_420.ics
old_url: http://www.csml.ucl.ac.uk/events/420
---
